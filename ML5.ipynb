{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHX9p5jfTySS"
      },
      "source": [
        "## Задание 5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnHNZtbXlH0"
      },
      "source": [
        "Набор данных тут: https://github.com/sismetanin/rureviews, также есть в папке [Data](https://drive.google.com/drive/folders/1YAMe7MiTxA-RSSd8Ex2p-L0Dspe6Gs4L). Те, кто предпочитает работать с английским языком, могут использовать набор данных `sms_spam`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJox-LoonoPx"
      },
      "source": [
        "Применим полученные навыки и решим задачу анализа тональности отзывов. \n",
        "\n",
        "Нужно повторить весь пайплайн от сырых текстов до получения обученной модели.\n",
        "\n",
        "Обязательные шаги предобработки:\n",
        "1. токенизация\n",
        "2. приведение к нижнему регистру\n",
        "3. удаление стоп-слов\n",
        "4. лемматизация\n",
        "5. векторизация (с настройкой гиперпараметров)\n",
        "6. построение модели\n",
        "7. оценка качества модели\n",
        "\n",
        "Обязательно использование векторайзеров:\n",
        "1. мешок n-грамм (диапазон для n подбирайте самостоятельно, запрещено использовать только униграммы).\n",
        "2. tf-idf ((диапазон для n подбирайте самостоятельно, также нужно подбирать параметры max_df, min_df, max_features)\n",
        "3. символьные n-граммы (диапазон для n подбирайте самостоятельно)\n",
        "\n",
        "В качестве классификатора нужно использовать наивный байесовский классификатор. \n",
        "\n",
        "Для сравнения векторайзеров между собой используйте precision, recall, f1-score и accuracy. Для этого сформируйте датафрейм, в котором в строках будут разные векторайзеры, а в столбцах разные метрики качества, а в  ячейках будут значения этих метрик для соответсвующих векторайзеров."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnNTp5VMqDo"
      },
      "source": [
        "1. Подключим все необходимое"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qikdaaj3CUtU",
        "outputId": "49b18254-d6c0-4b73-f95c-86b8aa8e1b54"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxX8IWOZDjZm",
        "outputId": "13378aec-9e1d-4100-930c-e46a8965da57"
      },
      "source": [
        "!pip install pymorphy2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 7.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-wHhGbHDDvH",
        "outputId": "bc77cfb4-2d7e-4391-f11d-001c55769a4b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk \n",
        "import string\n",
        "\n",
        "from sklearn.metrics import * \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "data = pd.read_csv('/content/drive/MyDrive/data/women-clothing-accessories.csv', sep='\\t', usecols=[0,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blaw-fu-Myji"
      },
      "source": [
        "2. Лемматизируем датасет и реализуем функцию составления отчета по результатам оценок"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dmxRgUxH44p"
      },
      "source": [
        "def lemmatize(x_data):\n",
        "  pymrth_analyzer = MorphAnalyzer()\n",
        "  sentences = []\n",
        "  \n",
        "  for j in range(len(x_data)):\n",
        "    sentence = x_data[j]\n",
        "\n",
        "    for ch in string.punctuation:\n",
        "      sentence = sentence.replace(ch,\"\")\n",
        "\n",
        "    sent = word_tokenize(sentence)\n",
        "    normal_sent = \"\"\n",
        "    \n",
        "    for i in range(len(sent)):\n",
        "      word = pymrth_analyzer.parse(sent[i])\n",
        "      normal_word = word[0].normal_form\n",
        "      normal_sent += (normal_word + \" \") \n",
        "\n",
        "    sentences.append(normal_sent)\n",
        "\n",
        "  return pd.Series(sentences)\n",
        "def make_report_df(report, vectorizer_name='no_name'):\n",
        "  comment_types = ['neautral', 'negative', 'positive']\n",
        "  score_types = ['precision', 'recall', 'f1-score']\n",
        "\n",
        "  result = {'accuracy': report['accuracy']}\n",
        "\n",
        "  for score in score_types:\n",
        "    avg = 0\n",
        "    for comment in comment_types:\n",
        "      avg = avg + report[comment][score]\n",
        "    avg = avg / len(comment_types)\n",
        "    result[score] = avg\n",
        "\n",
        "  return pd.DataFrame(data=result, index=[vectorizer_name])\n",
        "global_report_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_Ny-3JqIB-w",
        "outputId": "2a905952-4724-4ec9-cdcb-a82af93c4767"
      },
      "source": [
        "lemmatized_data_review = lemmatize(data.review)\n",
        "lemmatized_data_review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        качество плохой пошив ужасный горловина напере...\n",
              "1        товар отдать другой человек я не получить посы...\n",
              "2        ужасный синтетик тонкий ничего общий с предста...\n",
              "3        товар не прийти продавец продлить защита без м...\n",
              "4              кофточка голый синтетик носить не возможно \n",
              "                               ...                        \n",
              "89995    сделать достаточно хорошо на ткань сделать рис...\n",
              "89996    накидка шикарный спасибо большой провдо линять...\n",
              "89997    спасибо большой продовца рекомендовать заказат...\n",
              "89998    очень довольный заказ маленький месяц в рб кур...\n",
              "89999    хороший куртка посторонний запах нет шов ровны...\n",
              "Length: 90000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orY5bSoTNDdy"
      },
      "source": [
        "3. Разделим предобработанные данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi0gWYE3LWzx"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(lemmatized_data_review, data.sentiment, train_size = 0.7)\n",
        "noise = stopwords.words('russian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkcjggwNNLp8"
      },
      "source": [
        "4. Запустим подбор параметров для мешка n-грамм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "angL6oMjNKxf"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "           ('vect', CountVectorizer(tokenizer=word_tokenize, stop_words=noise)),\n",
        "           ('clf', MultinomialNB()),\n",
        "])\n",
        "parameters = [{\n",
        "    'vect__ngram_range': ((1,2), (1,3), (2,2), (2,3), (3,3))  \n",
        "}]\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, cv=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_n, best_m = grid_search.best_params_['vect__ngram_range']\n",
        "\n",
        "print(\"( \", best_n, \" , \", best_m, \" ) : best parameters for n-gramms bag\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12X679jmXdcr"
      },
      "source": [
        "best_n, best_m = 1, 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnHhlcqnaViT"
      },
      "source": [
        "5. Применим их"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rRGl6N_QF5n"
      },
      "source": [
        "vectorizer = CountVectorizer(tokenizer=word_tokenize, ngram_range=(best_n, best_m), stop_words=noise)\n",
        "vectorized_x_train = vectorizer.fit_transform(X_train)\n",
        "model = MultinomialNB()\n",
        "model.fit(vectorized_x_train, y_train)\n",
        "vectorized_x_test = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kk4AEzdTmwS"
      },
      "source": [
        "pred = model.predict(vectorized_x_test)\n",
        "vect_compare = classification_report(y_test, pred, output_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGiH3x52agDL"
      },
      "source": [
        "6. Запустим подбор параметров для tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjapleT3UCSC",
        "outputId": "dd81a40a-e76a-488b-ac71-7d312c47e8fd"
      },
      "source": [
        "pipeline_1 = Pipeline([\n",
        "           ('vect', TfidfVectorizer(tokenizer=word_tokenize, stop_words=noise)),\n",
        "           ('clf', MultinomialNB()),\n",
        "])\n",
        "parameters_1 = [{\n",
        "    'vect__ngram_range': ((1,2), (1,3), (2,2), (2,3), (3,3)),\n",
        "    'vect__max_df': [0.2 , 0.4, 0.6, 0.8, 1.0],\n",
        "    'vect__min_df': [0.  , 0.2, 0.4, 0.6, 0.8, 1.0],\n",
        "    'vect__max_features': [50000, 100000, 150000]\n",
        "}]\n",
        "\n",
        "grid_search_Tfidf = GridSearchCV(pipeline_1, parameters_1, n_jobs=-1, cv=2, verbose=10)\n",
        "grid_search_Tfidf.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 450 candidates, totalling 900 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   25.4s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   53.2s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  6.0min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  7.2min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  9.1min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed: 10.7min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed: 12.1min\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed: 13.7min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed: 15.7min\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 18.0min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 19.9min\n",
            "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 21.8min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 25.2min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 27.4min\n",
            "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed: 29.7min\n",
            "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed: 33.4min\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 36.0min\n",
            "[Parallel(n_jobs=-1)]: Done 309 tasks      | elapsed: 39.5min\n",
            "[Parallel(n_jobs=-1)]: Done 334 tasks      | elapsed: 42.7min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 45.6min\n",
            "[Parallel(n_jobs=-1)]: Done 388 tasks      | elapsed: 49.8min\n",
            "[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed: 52.8min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 57.4min\n",
            "[Parallel(n_jobs=-1)]: Done 477 tasks      | elapsed: 60.6min\n",
            "[Parallel(n_jobs=-1)]: Done 508 tasks      | elapsed: 65.4min\n",
            "[Parallel(n_jobs=-1)]: Done 541 tasks      | elapsed: 69.0min\n",
            "[Parallel(n_jobs=-1)]: Done 574 tasks      | elapsed: 73.9min\n",
            "[Parallel(n_jobs=-1)]: Done 609 tasks      | elapsed: 78.5min\n",
            "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed: 82.8min\n",
            "[Parallel(n_jobs=-1)]: Done 681 tasks      | elapsed: 88.0min\n",
            "[Parallel(n_jobs=-1)]: Done 718 tasks      | elapsed: 92.1min\n",
            "[Parallel(n_jobs=-1)]: Done 757 tasks      | elapsed: 97.6min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed: 103.2min\n",
            "[Parallel(n_jobs=-1)]: Done 837 tasks      | elapsed: 107.6min\n",
            "[Parallel(n_jobs=-1)]: Done 878 tasks      | elapsed: 113.5min\n",
            "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 115.8min finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=2, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words=['и', 'в',\n",
              "                                                                    'во', '...\n",
              "                                        MultinomialNB(alpha=1.0,\n",
              "                                                      class_prior=None,\n",
              "                                                      fit_prior=True))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid=[{'vect__max_df': [0.2, 0.4, 0.6, 0.8, 1.0],\n",
              "                          'vect__max_features': [50000, 100000, 150000],\n",
              "                          'vect__min_df': [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
              "                          'vect__ngram_range': ((1, 2), (1, 3), (2, 2), (2, 3),\n",
              "                                                (3, 3))}],\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=10)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8YL8rlMt2P0",
        "outputId": "1f8cc366-1f11-4494-8b17-87e173063db2"
      },
      "source": [
        "best_n, best_m = grid_search_Tfidf.best_params_['vect__ngram_range']\n",
        "best_max_df = grid_search_Tfidf.best_params_['vect__max_df']\n",
        "best_min_df = grid_search_Tfidf.best_params_['vect__min_df']\n",
        "best_max_features = grid_search_Tfidf.best_params_['vect__max_features']\n",
        "\n",
        "print(\"( \", best_n, \" , \", best_m, \" ) : best parameters for Tfidf\")\n",
        "print(best_max_df, \" \", best_min_df, \" \", best_max_features, \" : best max_df, min_df and max_features for Tfidf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(  1  ,  2  ) : best parameters for Tfidf\n",
            "0.2   0.0   150000  : best max_df, min_df and max_features for Tfidf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB1ZfgsI03fU"
      },
      "source": [
        "Значения параметра max_features больше 150_000 вызывают переобучение, а меньше - модель недообучается"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2-lGpt3RFZD"
      },
      "source": [
        "best_n, best_m, best_max_df, best_min_df, best_max_features = 1, 2, 0.2, 0.0, 150000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sSfLodpatY4"
      },
      "source": [
        "7. Применим их"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrw1tfLQNbB5"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(best_n, best_m), tokenizer=word_tokenize, stop_words=noise, max_df=best_max_df, min_df=best_min_df, max_features=best_max_features)\n",
        "tfidf_vectorized_x_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(tfidf_vectorized_x_train, y_train)\n",
        "\n",
        "tfidf_vectorized_x_test = tfidf_vectorizer.transform(X_test)\n",
        "pred = clf.predict(tfidf_vectorized_x_test)\n",
        "report_tfidf = classification_report(y_test, pred, output_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StKd_4uiaxhn"
      },
      "source": [
        "8. Запустим подбор параметров для символьных n-грамм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUmxlzpFOKDc",
        "outputId": "21c8e2a8-1693-4eaf-8a43-ea373debb9bc"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "           ('vect', CountVectorizer(analyzer='char')),\n",
        "           ('clf', MultinomialNB()),\n",
        "])\n",
        "parameters = [{\n",
        "    'vect__ngram_range': ((3,6), (3,8), (3,10), (4,8), (4,10), (5,9), (5,11), (6,12))  \n",
        "}]\n",
        "\n",
        "grid_search_char = GridSearchCV(pipeline, parameters, n_jobs=-1, cv=2, verbose=10)\n",
        "grid_search_char.fit(X_train, y_train)\n",
        "best_n, best_m = grid_search_char.best_params_['vect__ngram_range']\n",
        "best_n, best_m\n",
        "\n",
        "print(\"( \", best_n, \" , \", best_m, \" ) : best parameters for char vectorizer\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   51.8s\n",
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  8.0min\n",
            "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed: 14.3min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(  5  ,  11  ) : best parameters for char vectorizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfkV5m9FWBXY"
      },
      "source": [
        "best_n, best_m = 5, 11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct2Lh_lobCQs"
      },
      "source": [
        "9. Применим их"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkn6M3FxY4xB"
      },
      "source": [
        "char_vectorizer = CountVectorizer(ngram_range=(best_n, best_m), analyzer='char')\n",
        "char_vectorized_x_train = char_vectorizer.fit_transform(X_train)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(char_vectorized_x_train, y_train)\n",
        "char_vectorized_x_test = char_vectorizer.transform(X_test)\n",
        "pred = clf.predict(char_vectorized_x_test)\n",
        "report_char_ngram = classification_report(y_test, pred, output_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8dW3MvkbHd5"
      },
      "source": [
        "Составим таблицу усредненных по количеству sentiment оценок"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "JUySeeNXOPXd",
        "outputId": "4cb350f2-b10c-4f82-effc-c817df350bca"
      },
      "source": [
        "global_report_df = pd.DataFrame()\n",
        "global_report_df = global_report_df.append(make_report_df(vect_compare, \"n-gramms bag\"))\n",
        "global_report_df = global_report_df.append(make_report_df(report_tfidf, \"tf-idf\"))\n",
        "global_report_df = global_report_df.append(make_report_df(report_char_ngram, \"char n-gram\"))\n",
        "global_report_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>n-gramms bag</th>\n",
              "      <td>0.710788</td>\n",
              "      <td>0.711183</td>\n",
              "      <td>0.711672</td>\n",
              "      <td>0.710905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tf-idf</th>\n",
              "      <td>0.711196</td>\n",
              "      <td>0.712609</td>\n",
              "      <td>0.712023</td>\n",
              "      <td>0.711462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>char n-gram</th>\n",
              "      <td>0.733639</td>\n",
              "      <td>0.738406</td>\n",
              "      <td>0.734321</td>\n",
              "      <td>0.735465</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              accuracy  precision    recall  f1-score\n",
              "n-gramms bag  0.710788   0.711183  0.711672  0.710905\n",
              "tf-idf        0.711196   0.712609  0.712023  0.711462\n",
              "char n-gram   0.733639   0.738406  0.734321  0.735465"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUGG36upbY_P"
      },
      "source": [
        "Составим полную таблицу оценок по всем sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "lnZsSzX_Zj-l",
        "outputId": "f368274e-a38d-4596-b954-45d5071ff655"
      },
      "source": [
        "import itertools\n",
        "global_report_df = pd.DataFrame()\n",
        "vect_compare = pd.DataFrame().from_dict(dict(itertools.islice(vect_compare.items(), 4)))\n",
        "vect_compare['name'] = 'n-gramm bag'\n",
        "global_report_df = global_report_df.append(vect_compare[:-1])\n",
        "vect_compare = pd.DataFrame().from_dict(dict(itertools.islice(report_tfidf.items(), 4)))\n",
        "vect_compare['name'] = 'tf-idf'\n",
        "global_report_df = global_report_df.append(vect_compare[:-1])\n",
        "vect_compare = pd.DataFrame().from_dict(dict(itertools.islice(report_char_ngram.items(), 4)))\n",
        "vect_compare['name'] = 'char'\n",
        "global_report_df = global_report_df.append(vect_compare[:-1])\n",
        "global_report_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>neautral</th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.602708</td>\n",
              "      <td>0.702955</td>\n",
              "      <td>0.827753</td>\n",
              "      <td>0.710788</td>\n",
              "      <td>n-gramm bag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.629405</td>\n",
              "      <td>0.654232</td>\n",
              "      <td>0.847273</td>\n",
              "      <td>0.710788</td>\n",
              "      <td>n-gramm bag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.615767</td>\n",
              "      <td>0.677719</td>\n",
              "      <td>0.837399</td>\n",
              "      <td>0.710788</td>\n",
              "      <td>n-gramm bag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.600479</td>\n",
              "      <td>0.704904</td>\n",
              "      <td>0.831453</td>\n",
              "      <td>0.710974</td>\n",
              "      <td>tf-idf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.637026</td>\n",
              "      <td>0.649611</td>\n",
              "      <td>0.844738</td>\n",
              "      <td>0.710974</td>\n",
              "      <td>tf-idf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.618213</td>\n",
              "      <td>0.676129</td>\n",
              "      <td>0.838043</td>\n",
              "      <td>0.710974</td>\n",
              "      <td>tf-idf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.615270</td>\n",
              "      <td>0.720882</td>\n",
              "      <td>0.881179</td>\n",
              "      <td>0.733899</td>\n",
              "      <td>char</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.683641</td>\n",
              "      <td>0.659867</td>\n",
              "      <td>0.856419</td>\n",
              "      <td>0.733899</td>\n",
              "      <td>char</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.647656</td>\n",
              "      <td>0.689026</td>\n",
              "      <td>0.868623</td>\n",
              "      <td>0.733899</td>\n",
              "      <td>char</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           neautral  negative  positive  accuracy         name\n",
              "precision  0.602708  0.702955  0.827753  0.710788  n-gramm bag\n",
              "recall     0.629405  0.654232  0.847273  0.710788  n-gramm bag\n",
              "f1-score   0.615767  0.677719  0.837399  0.710788  n-gramm bag\n",
              "precision  0.600479  0.704904  0.831453  0.710974       tf-idf\n",
              "recall     0.637026  0.649611  0.844738  0.710974       tf-idf\n",
              "f1-score   0.618213  0.676129  0.838043  0.710974       tf-idf\n",
              "precision  0.615270  0.720882  0.881179  0.733899         char\n",
              "recall     0.683641  0.659867  0.856419  0.733899         char\n",
              "f1-score   0.647656  0.689026  0.868623  0.733899         char"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9DlUXsc1RBO"
      },
      "source": [
        "Исходя из полученных оценок можно сказать что лучший результат показал векторайзер символьные n-граммы. TF-IDF и мешок n-грамм сработали примерно одинаково."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYTwyMtWhAZ"
      },
      "source": [
        "## Задание 5.2 Регулярные выражения\n",
        "\n",
        "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах. \n",
        "\n",
        "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
        "\n",
        "Навык полезный, давайте в нём тоже потренируемся.\n",
        "\n",
        "Для работы с регулярными выражениями есть библиотека **re**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaUW5S4gWhAb"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6aYh7Osl8xr"
      },
      "source": [
        "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
        "* **?а** - ноль или один символ **а**\n",
        "* **+а** - один или более символов **а**\n",
        "* **\\*а** - ноль или более символов **а** (не путать с +)\n",
        "* **.** - любое количество любого символа\n",
        "\n",
        "Пример:\n",
        "Выражению \\*a?b. соответствуют последовательности a, ab, abc, aa, aac НО НЕ abb!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zOFFA3l_KQ"
      },
      "source": [
        "Рассмотрим подробно несколько наиболее полезных функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJrUpARWhAd"
      },
      "source": [
        "### findall\n",
        "возвращает список всех найденных непересекающихся совпадений.\n",
        "\n",
        "Регулярное выражение **ab+c.**: \n",
        "* **a** - просто символ **a**\n",
        "* **b+** - один или более символов **b**\n",
        "* **c** - просто символ **c**\n",
        "* **.** - любой символ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2athHzKuWhAd",
        "outputId": "a88c3010-5173-4459-c7b9-0de10a57c00d"
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abcd', 'abca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9FpIw5RWhAf"
      },
      "source": [
        "Вопрос на внимательность: почему нет abcx?\n",
        "Потому что сказано что findall возвращает список всех непересекающихся совпадений, а abcx пересекается с abca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ttzoxEWhAg"
      },
      "source": [
        "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZR2AEq3WhAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3baa6900-7e46-4f45-b328-9eac0159899e"
      },
      "source": [
        "stroka = 'Ваня вышел погулять, вновь увидел самокат'\n",
        "res = re.findall(r'\\b\\w{1,2}', stroka) \n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ва', 'вы', 'по', 'вн', 'ув', 'са']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI18l-l9WhAk"
      },
      "source": [
        "### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVKdRoc1WhAl",
        "outputId": "a13642dc-7efb-4a8a-eb57-9ae86701d677"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie', ' weenie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10u5efuSWhAm"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U9EQZMwWhAn",
        "outputId": "84576aa4-17d6-4165-9df9-64a1a714f8d2"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMcMyflWhAp"
      },
      "source": [
        "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgPSjEOWhAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614c5f6e-dc25-41a0-8dc4-0ae7a7746fdb"
      },
      "source": [
        "stroka = 'itsy. bitsy. teenie. weenie.'\n",
        "res = re.split(r'\\.+', stroka, maxsplit=2)\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie. weenie.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrEGqBSWhAr"
      },
      "source": [
        "### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az3KxKWwWhAr",
        "outputId": "8d473d85-9f31-4aee-9642-5b24405c7a70"
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bbcbbc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0n7_HPWhAt"
      },
      "source": [
        "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Sdu7xlWhAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de55eec8-cec1-470b-d470-bb687b656cc9"
      },
      "source": [
        "stroka = \"22*2=4 это база\"\n",
        "res = re.sub(r'[0-9]', 'DIG', stroka)\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIGDIG*DIG=DIG это база\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8__oi1PWhAv"
      },
      "source": [
        "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNS9zt4WhAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7030373f-9b8c-41c3-e1aa-f1d04caff70a"
      },
      "source": [
        "stroka = \"ссылка на видео https://www.youtube.com/watch?v=i0tfiaIXcBE\"\n",
        "res = re.sub(r'(s|ht|f)(m?)(tp?)(s?):\\S+', '', stroka)\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ссылка на видео \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStgBJy2WhAx"
      },
      "source": [
        "### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JstTupisWhAy",
        "outputId": "a443dbab-bc97-4b60-c7b7-8e8dc58f70dd"
      },
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[\\w\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEXc3G0WhA2"
      },
      "source": [
        "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFvnIWbUWhA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cae931a-756f-4b54-81b0-7075a83e8c3c"
      },
      "source": [
        "prog = re.compile(r'[\\b\\w-]{4,}')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'больше', 'больше', 'слов', 'Что-то']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDNZ3HQWhA3"
      },
      "source": [
        "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
        "\n",
        "```\n",
        "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pr5fYOCeM3K",
        "outputId": "96e2fb55-96d8-4a1e-e267-3b46f8108aaa"
      },
      "source": [
        "prog = re.compile(r'@\\w+.\\w+')\n",
        "prog.findall('abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}